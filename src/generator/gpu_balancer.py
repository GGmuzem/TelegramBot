"""
–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ –Ω–∞–≥—Ä—É–∑–∫–∏ GPU –¥–ª—è AI –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
"""
import threading
import logging
import asyncio
from typing import Dict, Any, List, Optional
from datetime import datetime

import torch

from src.shared.config import settings
from src.shared.redis_client import redis_client

logger = logging.getLogger(__name__)


class GPUBalancer:
    """–ë–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫ –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É GPU –¥–ª—è –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è RTX 5080"""
    
    def __init__(self):
        # –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU
        self.gpu_devices = self._get_gpu_devices()
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ –∫–∞–∂–¥–æ–≥–æ GPU
        self.gpu_status = {}
        
        # –ë–ª–æ–∫–∏—Ä–æ–≤–∫–∞ –¥–ª—è thread-safety
        self.lock = threading.Lock()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ GPU
        self._initialize_gpu_status()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è
        self.stats = {
            'total_generations': 0,
            'gpu_usage_time': {},
            'generation_times': [],
            'error_count': 0
        }
    
    def _get_gpu_devices(self) -> List[int]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö GPU"""
        try:
            if not torch.cuda.is_available():
                logger.warning("CUDA –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞")
                return []
            
            gpu_count = torch.cuda.device_count()
            logger.info(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–æ GPU: {gpu_count}")
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é GPU –∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–∫
            if hasattr(settings, 'GPU_DEVICES') and settings.GPU_DEVICES:
                configured_devices = [int(x) for x in settings.GPU_DEVICES.split(',')]
                available_devices = [i for i in configured_devices if i < gpu_count]
            else:
                available_devices = list(range(gpu_count))
            
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º RTX 5080 –∏ –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä—É Blackwell
            validated_devices = []
            for gpu_id in available_devices:
                props = torch.cuda.get_device_properties(gpu_id)
                
                logger.info(
                    f"GPU {gpu_id}: {props.name}, "
                    f"Compute: {props.major}.{props.minor}, "
                    f"Memory: {props.total_memory / 1024**3:.1f} GB"
                )
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–µ —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è
                if props.total_memory >= 8 * 1024**3:  # –ú–∏–Ω–∏–º—É–º 8GB
                    validated_devices.append(gpu_id)
                    
                    # –û—Ç–º–µ—á–∞–µ–º RTX 5080 —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Blackwell
                    if "RTX 5080" in props.name or (props.major == 12 and props.minor == 0):
                        logger.info(f"üöÄ RTX 5080 —Å –∞—Ä—Ö–∏—Ç–µ–∫—Ç—É—Ä–æ–π Blackwell –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –Ω–∞ GPU {gpu_id}")
                else:
                    logger.warning(f"GPU {gpu_id} –Ω–µ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É–µ—Ç –º–∏–Ω–∏–º–∞–ª—å–Ω—ã–º —Ç—Ä–µ–±–æ–≤–∞–Ω–∏—è–º")
            
            return validated_devices
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è GPU: {e}")
            return []
    
    def _initialize_gpu_status(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å—Ç–∞—Ç—É—Å–∞ GPU"""
        for gpu_id in self.gpu_devices:
            self.gpu_status[gpu_id] = {
                "busy": False,
                "queue_length": 0,
                "current_task": None,
                "total_generations": 0,
                "total_time": 0.0,
                "error_count": 0,
                "memory_usage": 0.0,
                "temperature": 0.0,
                "last_used": None
            }
        
        logger.info(f"–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω—ã GPU: {list(self.gpu_status.keys())}")
    
    async def get_available_gpu(self, priority: bool = False) -> Optional[int]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –æ–ø—Ç–∏–º–∞–ª—å–Ω–æ–≥–æ GPU –¥–ª—è –∑–∞–¥–∞—á–∏
        
        Args:
            priority: –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω–∞—è –∑–∞–¥–∞—á–∞ (–¥–ª—è –ø—Ä–µ–º–∏—É–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π)
        
        Returns:
            ID –¥–æ—Å—Ç—É–ø–Ω–æ–≥–æ GPU –∏–ª–∏ None
        """
        try:
            with self.lock:
                available_gpus = []
                
                # –°–Ω–∞—á–∞–ª–∞ –∏—â–µ–º –ø–æ–ª–Ω–æ—Å—Ç—å—é —Å–≤–æ–±–æ–¥–Ω—ã–µ GPU
                for gpu_id, status in self.gpu_status.items():
                    if not status["busy"] and status["queue_length"] == 0:
                        available_gpus.append((gpu_id, 0))
                
                # –ï—Å–ª–∏ –Ω–µ—Ç —Å–≤–æ–±–æ–¥–Ω—ã—Ö, –∏—â–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ—á–µ—Ä–µ–¥—å—é
                if not available_gpus:
                    for gpu_id, status in self.gpu_status.items():
                        if not status["busy"]:
                            available_gpus.append((gpu_id, status["queue_length"]))
                
                # –ï—Å–ª–∏ –≤—Å–µ –∑–∞–Ω—è—Ç—ã, –≤—ã–±–∏—Ä–∞–µ–º —Å –º–∏–Ω–∏–º–∞–ª—å–Ω–æ–π –æ—á–µ—Ä–µ–¥—å—é
                if not available_gpus:
                    for gpu_id, status in self.gpu_status.items():
                        available_gpus.append((gpu_id, status["queue_length"]))
                
                if not available_gpus:
                    return None
                
                # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ –Ω–∞–≥—Ä—É–∑–∫–µ –∏ –≤—ã–±–∏—Ä–∞–µ–º –æ–ø—Ç–∏–º–∞–ª—å–Ω—ã–π
                available_gpus.sort(key=lambda x: (x[1], self.gpu_status[x[0]]["total_generations"]))
                
                selected_gpu = available_gpus[0][0]
                
                # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å
                if priority:
                    # –ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç–Ω—ã–µ –∑–∞–¥–∞—á–∏ –∏–¥—É—Ç –≤ –Ω–∞—á–∞–ª–æ –æ—á–µ—Ä–µ–¥–∏
                    self.gpu_status[selected_gpu]["busy"] = True
                    self.gpu_status[selected_gpu]["current_task"] = f"priority_{datetime.now().isoformat()}"
                else:
                    self.gpu_status[selected_gpu]["queue_length"] += 1
                
                logger.info(f"–í—ã–¥–µ–ª–µ–Ω GPU {selected_gpu} (–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç: {priority})")
                return selected_gpu
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤—ã–±–æ—Ä–∞ GPU: {e}")
            return None
    
    def release_gpu(self, gpu_id: int, generation_time: float = 0.0):
        """
        –û—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏–µ GPU –ø–æ—Å–ª–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∏—è –∑–∞–¥–∞—á–∏
        
        Args:
            gpu_id: ID GPU –¥–ª—è –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è
            generation_time: –í—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ —Å–µ–∫—É–Ω–¥–∞—Ö
        """
        try:
            with self.lock:
                if gpu_id in self.gpu_status:
                    status = self.gpu_status[gpu_id]
                    
                    if status["queue_length"] > 0:
                        status["queue_length"] -= 1
                    else:
                        status["busy"] = False
                        status["current_task"] = None
                    
                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
                    status["total_generations"] += 1
                    status["total_time"] += generation_time
                    status["last_used"] = datetime.now()
                    
                    if generation_time > 0:
                        self.stats["generation_times"].append(generation_time)
                        self.stats["total_generations"] += 1
                    
                    logger.info(f"–û—Å–≤–æ–±–æ–∂–¥–µ–Ω GPU {gpu_id}, –≤—Ä–µ–º—è: {generation_time:.1f}—Å")
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Å–≤–æ–±–æ–∂–¥–µ–Ω–∏—è GPU {gpu_id}: {e}")
    
    def mark_gpu_error(self, gpu_id: int, error: str):
        """–û—Ç–º–µ—Ç–∫–∞ –æ—à–∏–±–∫–∏ –Ω–∞ GPU"""
        try:
            with self.lock:
                if gpu_id in self.gpu_status:
                    self.gpu_status[gpu_id]["error_count"] += 1
                    self.stats["error_count"] += 1
                    
                    logger.error(f"–û—à–∏–±–∫–∞ –Ω–∞ GPU {gpu_id}: {error}")
                    
                    # –ï—Å–ª–∏ –º–Ω–æ–≥–æ –æ—à–∏–±–æ–∫, –≤—Ä–µ–º–µ–Ω–Ω–æ –∏—Å–∫–ª—é—á–∞–µ–º GPU
                    if self.gpu_status[gpu_id]["error_count"] > 5:
                        logger.warning(f"GPU {gpu_id} –≤—Ä–µ–º–µ–Ω–Ω–æ –æ—Ç–∫–ª—é—á–µ–Ω –∏–∑-–∑–∞ –º–Ω–æ–∂–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –æ—à–∏–±–æ–∫")
                        # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ –æ—Ç–∫–ª—é—á–µ–Ω–∏—è
                        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–º–µ—Ç–∫–∏ –æ—à–∏–±–∫–∏ GPU {gpu_id}: {e}")
    
    async def get_gpu_stats(self) -> Dict[str, Any]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è GPU"""
        try:
            with self.lock:
                stats = {
                    "gpu_count": len(self.gpu_devices),
                    "gpu_status": self.gpu_status.copy(),
                    "total_generations": self.stats["total_generations"],
                    "total_errors": self.stats["error_count"],
                    "average_generation_time": 0.0,
                    "queue_lengths": {},
                    "memory_usage": {},
                    "availability": {}
                }
                
                # –í—ã—á–∏—Å–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏
                if self.stats["generation_times"]:
                    stats["average_generation_time"] = sum(self.stats["generation_times"]) / len(self.stats["generation_times"])
                
                # –ü–æ–ª—É—á–∞–µ–º —Ç–µ–∫—É—â—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ GPU
                for gpu_id in self.gpu_devices:
                    try:
                        # –î–ª–∏–Ω–∞ –æ—á–µ—Ä–µ–¥–∏
                        stats["queue_lengths"][gpu_id] = self.gpu_status[gpu_id]["queue_length"]
                        
                        # –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ GPU
                        if torch.cuda.is_available():
                            torch.cuda.set_device(gpu_id)
                            memory_allocated = torch.cuda.memory_allocated(gpu_id) / 1024**3
                            memory_reserved = torch.cuda.memory_reserved(gpu_id) / 1024**3
                            
                            stats["memory_usage"][gpu_id] = {
                                "allocated_gb": round(memory_allocated, 2),
                                "reserved_gb": round(memory_reserved, 2)
                            }
                        
                        # –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å
                        stats["availability"][gpu_id] = not self.gpu_status[gpu_id]["busy"]
                        
                    except Exception as e:
                        logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É GPU {gpu_id}: {e}")
                
                return stats
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ GPU: {e}")
            return {"error": str(e)}
    
    async def optimize_gpu_assignment(self):
        """–û–ø—Ç–∏–º–∏–∑–∞—Ü–∏—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è –Ω–∞–≥—Ä—É–∑–∫–∏ –º–µ–∂–¥—É GPU"""
        try:
            with self.lock:
                # –ù–∞—Ö–æ–¥–∏–º –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω–Ω—ã–µ –∏ –Ω–µ–¥–æ–≥—Ä—É–∂–µ–Ω–Ω—ã–µ GPU
                total_load = sum(status["queue_length"] for status in self.gpu_status.values())
                
                if total_load == 0:
                    return
                
                average_load = total_load / len(self.gpu_devices)
                
                overloaded_gpus = []
                underloaded_gpus = []
                
                for gpu_id, status in self.gpu_status.items():
                    if status["queue_length"] > average_load * 1.5:
                        overloaded_gpus.append(gpu_id)
                    elif status["queue_length"] < average_load * 0.5:
                        underloaded_gpus.append(gpu_id)
                
                # –õ–æ–≥–∏—Ä—É–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞
                if overloaded_gpus or underloaded_gpus:
                    logger.info(
                        f"GPU –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∞: –ø–µ—Ä–µ–≥—Ä—É–∂–µ–Ω—ã {overloaded_gpus}, "
                        f"–Ω–µ–¥–æ–≥—Ä—É–∂–µ–Ω—ã {underloaded_gpus}"
                    )
                
                # –ú–æ–∂–Ω–æ –¥–æ–±–∞–≤–∏—Ç—å –ª–æ–≥–∏–∫—É –ø–µ—Ä–µ–±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤–∫–∏ –∑–∞–¥–∞—á
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ GPU: {e}")
    
    async def cleanup_stale_tasks(self):
        """–û—á–∏—Å—Ç–∫–∞ –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á"""
        try:
            with self.lock:
                current_time = datetime.now()
                
                for gpu_id, status in self.gpu_status.items():
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∑–∞–¥–∞—á–∏, –∫–æ—Ç–æ—Ä—ã–µ –≤—ã–ø–æ–ª–Ω—è—é—Ç—Å—è —Å–ª–∏—à–∫–æ–º –¥–æ–ª–≥–æ
                    if (status["current_task"] and 
                        status["last_used"] and 
                        (current_time - status["last_used"]).seconds > 600):  # 10 –º–∏–Ω—É—Ç
                        
                        logger.warning(f"–û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –∑–∞–≤–∏—Å—à–∞—è –∑–∞–¥–∞—á–∞ –Ω–∞ GPU {gpu_id}")
                        
                        # –û—Å–≤–æ–±–æ–∂–¥–∞–µ–º GPU
                        status["busy"] = False
                        status["current_task"] = None
                        status["error_count"] += 1
                        
                        # –û—á–∏—â–∞–µ–º –ø–∞–º—è—Ç—å GPU
                        try:
                            torch.cuda.empty_cache()
                        except:
                            pass
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—á–∏—Å—Ç–∫–∏ –∑–∞–≤–∏—Å—à–∏—Ö –∑–∞–¥–∞—á: {e}")
    
    async def save_stats_to_redis(self):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ Redis –¥–ª—è –º–æ–Ω–∏—Ç–æ—Ä–∏–Ω–≥–∞"""
        try:
            stats = await self.get_gpu_stats()
            await redis_client.set(
                "gpu_balancer_stats",
                stats,
                ex=300  # 5 –º–∏–Ω—É—Ç
            )
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ GPU –≤ Redis: {e}")
    
    def get_recommended_quality(self, gpu_id: int) -> str:
        """–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è –∫–∞—á–µ—Å—Ç–≤–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–≥—Ä—É–∑–∫–∏ GPU"""
        try:
            with self.lock:
                if gpu_id not in self.gpu_status:
                    return "standard"
                
                status = self.gpu_status[gpu_id]
                queue_length = status["queue_length"]
                
                # –†–µ–∫–æ–º–µ–Ω–¥—É–µ–º –∫–∞—á–µ—Å—Ç–≤–æ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –Ω–∞–≥—Ä—É–∑–∫–∏
                if queue_length == 0:
                    return "ultra"
                elif queue_length <= 2:
                    return "high"
                elif queue_length <= 5:
                    return "standard"
                else:
                    return "fast"
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º–æ–≥–æ –∫–∞—á–µ—Å—Ç–≤–∞: {e}")
            return "standard"


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä –±–∞–ª–∞–Ω—Å–∏—Ä–æ–≤—â–∏–∫–∞
gpu_balancer = GPUBalancer()